{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6080e5e-8cfa-4562-880e-23ba73331ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Scraped abhayvarn, messages: 200\n",
      "‚úÖ Scraped eqwires, messages: 200\n",
      "‚úÖ Scraped VGSTOCKRESEARCH, messages: 200\n",
      "‚úÖ Scraped EverydayProfits, messages: 200\n",
      "‚úÖ Scraped StockPro_Online, messages: 200\n",
      "‚úÖ Scraped ChaseAlpha, messages: 200\n",
      "‚úÖ Scraped Equity99, messages: 200\n",
      "‚úÖ Scraped SharesNservices, messages: 200\n",
      "‚úÖ Scraped DeltaTrading1, messages: 200\n",
      "‚úÖ Scraped PatelWealth, messages: 200\n",
      "‚úÖ Scraped STOCKGAINERSS, messages: 200\n",
      "‚úÖ Scraped PATELWEALTH, messages: 200\n",
      "‚úÖ Scraped Intradat, messages: 200\n",
      "‚úÖ Scraped GREEN_TRADERS_SEBI, messages: 200\n",
      "‚úÖ Scraped AngelOneAdvisory, messages: 200\n",
      "‚úÖ Scraped GapUp, messages: 200\n",
      "‚úÖ Scraped Centrum, messages: 0\n",
      "‚úÖ Scraped MotilalOswal, messages: 38\n",
      "‚úÖ Scraped LIVELONGWEALTH, messages: 200\n",
      "‚úÖ Scraped StockPhoenix, messages: 200\n",
      "‚úÖ Scraped Vision_Optiontrading, messages: 200\n",
      "‚úÖ Scraped OptionsGurukul, messages: 4\n",
      "‚úÖ Scraped DarkHorseOfStockMarket, messages: 200\n",
      "‚úÖ Scraped IntradayMatchSebiRegistered, messages: 200\n",
      "‚úÖ Scraped Sharekhan_Official, messages: 38\n",
      "‚úÖ Scraped CAJagdeesh, messages: 200\n",
      "‚úÖ Scraped TheOriginalBull, messages: 0\n",
      "‚úÖ Scraped TheFinberg, messages: 200\n",
      "‚úÖ Scraped PowerOfStocks, messages: 200\n",
      "‚úÖ Scraped LiveTradingTricks, messages: 200\n",
      "üíæ Saved to D:\\Darryl\\Coding\\SEBI_Project\\data\\raw\\sebi_groups_messages.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darryl\\AppData\\Local\\Temp\\ipykernel_13124\\3037805212.py:84: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_all = pd.concat(all_data, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_id</th>\n",
       "      <th>date</th>\n",
       "      <th>chat_id</th>\n",
       "      <th>sender_id</th>\n",
       "      <th>sender_username</th>\n",
       "      <th>sender_first_name</th>\n",
       "      <th>sender_last_name</th>\n",
       "      <th>sender_phone</th>\n",
       "      <th>text</th>\n",
       "      <th>views</th>\n",
       "      <th>forwards</th>\n",
       "      <th>reply_to_msg_id</th>\n",
       "      <th>media_type</th>\n",
       "      <th>has_hyperlink</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19552.0</td>\n",
       "      <td>2025-09-02 04:54:46+00:00</td>\n",
       "      <td>1.246175e+09</td>\n",
       "      <td>1.246175e+09</td>\n",
       "      <td>abhayvarn</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>47+ ü•≥üòçüíπü§©üòç\\nFirst Big Target done ü•≥üòçüíπ\\nMore tha...</td>\n",
       "      <td>3959.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19550.0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>abhayvarn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19551.0</td>\n",
       "      <td>2025-09-02 04:31:22+00:00</td>\n",
       "      <td>1.246175e+09</td>\n",
       "      <td>1.246175e+09</td>\n",
       "      <td>abhayvarn</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>33+ ü•≥üòçüíπü§©üòç\\nMore than 40% Returns coming now ‚úåÔ∏è...</td>\n",
       "      <td>4406.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19550.0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>abhayvarn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19550.0</td>\n",
       "      <td>2025-09-02 04:24:35+00:00</td>\n",
       "      <td>1.246175e+09</td>\n",
       "      <td>1.246175e+09</td>\n",
       "      <td>abhayvarn</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Hero Zero \\n\\nBuy Nifty 24750 CE @ 23 - 28\\n\\n...</td>\n",
       "      <td>4656.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>abhayvarn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19548.0</td>\n",
       "      <td>2025-09-02 04:07:17+00:00</td>\n",
       "      <td>1.246175e+09</td>\n",
       "      <td>1.246175e+09</td>\n",
       "      <td>abhayvarn</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.youtube.com/live/SUodqXwmxMs?si=Ho...</td>\n",
       "      <td>4830.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>webpage</td>\n",
       "      <td>True</td>\n",
       "      <td>abhayvarn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19547.0</td>\n",
       "      <td>2025-09-02 02:27:26+00:00</td>\n",
       "      <td>1.246175e+09</td>\n",
       "      <td>1.246175e+09</td>\n",
       "      <td>abhayvarn</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Good Newzz before Nifty Weekly Expiry ü§©üí•üî•\\n\\nO...</td>\n",
       "      <td>4974.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>webpage</td>\n",
       "      <td>True</td>\n",
       "      <td>abhayvarn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   message_id                      date       chat_id     sender_id  \\\n",
       "0     19552.0 2025-09-02 04:54:46+00:00  1.246175e+09  1.246175e+09   \n",
       "1     19551.0 2025-09-02 04:31:22+00:00  1.246175e+09  1.246175e+09   \n",
       "2     19550.0 2025-09-02 04:24:35+00:00  1.246175e+09  1.246175e+09   \n",
       "3     19548.0 2025-09-02 04:07:17+00:00  1.246175e+09  1.246175e+09   \n",
       "4     19547.0 2025-09-02 02:27:26+00:00  1.246175e+09  1.246175e+09   \n",
       "\n",
       "  sender_username sender_first_name sender_last_name sender_phone  \\\n",
       "0       abhayvarn              None             None         None   \n",
       "1       abhayvarn              None             None         None   \n",
       "2       abhayvarn              None             None         None   \n",
       "3       abhayvarn              None             None         None   \n",
       "4       abhayvarn              None             None         None   \n",
       "\n",
       "                                                text   views  forwards  \\\n",
       "0  47+ ü•≥üòçüíπü§©üòç\\nFirst Big Target done ü•≥üòçüíπ\\nMore tha...  3959.0       0.0   \n",
       "1  33+ ü•≥üòçüíπü§©üòç\\nMore than 40% Returns coming now ‚úåÔ∏è...  4406.0       0.0   \n",
       "2  Hero Zero \\n\\nBuy Nifty 24750 CE @ 23 - 28\\n\\n...  4656.0       3.0   \n",
       "3  https://www.youtube.com/live/SUodqXwmxMs?si=Ho...  4830.0       0.0   \n",
       "4  Good Newzz before Nifty Weekly Expiry ü§©üí•üî•\\n\\nO...  4974.0       0.0   \n",
       "\n",
       "   reply_to_msg_id media_type has_hyperlink      group  \n",
       "0          19550.0       None         False  abhayvarn  \n",
       "1          19550.0       None         False  abhayvarn  \n",
       "2              NaN       None         False  abhayvarn  \n",
       "3              NaN    webpage          True  abhayvarn  \n",
       "4              NaN    webpage          True  abhayvarn  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Server closed the connection: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Error executing high-level request after reconnect: <class 'sqlite3.OperationalError'>: database is locked\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from telethon import TelegramClient\n",
    "from telethon.tl.types import MessageMediaPhoto, MessageMediaDocument, MessageMediaWebPage\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# ------------------------------\n",
    "# SETUP TELETHON CLIENT\n",
    "# ------------------------------\n",
    "api_id =     # your API ID\n",
    "api_hash = \"API_HASH\"\n",
    "\n",
    "client = TelegramClient(\"sebi_scraper_session\", api_id, api_hash)\n",
    "\n",
    "# ------------------------------\n",
    "# SCRAPER FUNCTION\n",
    "# ------------------------------\n",
    "async def scrape_group_messages(group_username, limit=200):\n",
    "    messages_data = []\n",
    "    entity = await client.get_entity(group_username)\n",
    "\n",
    "    async for msg in client.iter_messages(entity, limit=limit):\n",
    "        sender = await msg.get_sender()\n",
    "        \n",
    "        # Detect media type\n",
    "        if msg.media:\n",
    "            if isinstance(msg.media, MessageMediaPhoto):\n",
    "                media_type = \"photo\"\n",
    "            elif isinstance(msg.media, MessageMediaDocument):\n",
    "                media_type = \"document\"\n",
    "            elif isinstance(msg.media, MessageMediaWebPage):\n",
    "                media_type = \"webpage\"\n",
    "            else:\n",
    "                media_type = \"other\"\n",
    "        else:\n",
    "            media_type = None\n",
    "\n",
    "        has_hyperlink = bool(re.search(r\"http[s]?://\", msg.text or \"\"))\n",
    "\n",
    "        messages_data.append({\n",
    "            \"message_id\": msg.id,\n",
    "            \"date\": msg.date,\n",
    "            \"chat_id\": entity.id,\n",
    "            \"sender_id\": sender.id if sender else None,\n",
    "            \"sender_username\": getattr(sender, \"username\", None),\n",
    "            \"sender_first_name\": getattr(sender, \"first_name\", None),\n",
    "            \"sender_last_name\": getattr(sender, \"last_name\", None),\n",
    "            \"sender_phone\": getattr(sender, \"phone\", None),\n",
    "            \"text\": msg.text,\n",
    "            \"views\": getattr(msg, \"views\", None),\n",
    "            \"forwards\": getattr(msg, \"forwards\", None),\n",
    "            \"reply_to_msg_id\": getattr(msg, \"reply_to_msg_id\", None),\n",
    "            \"media_type\": media_type,\n",
    "            \"has_hyperlink\": has_hyperlink,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(messages_data)\n",
    "\n",
    "# ------------------------------\n",
    "# MAIN SCRAPER LOOP\n",
    "# ------------------------------\n",
    "async def main():\n",
    "    group_usernames = [\n",
    "        \"abhayvarn\", \"eqwires\", \"VGSTOCKRESEARCH\", \"EverydayProfits\", \"StockPro_Online\",\n",
    "        \"ChaseAlpha\", \"Equity99\", \"SharesNservices\", \"DeltaTrading1\", \"PatelWealth\",\n",
    "        \"STOCKGAINERSS\", \"PATELWEALTH\", \"Intradat\", \"GREEN_TRADERS_SEBI\", \"AngelOneAdvisory\",\n",
    "        \"GapUp\", \"Centrum\", \"MotilalOswal\", \"LIVELONGWEALTH\", \"StockPhoenix\",\n",
    "        \"Vision_Optiontrading\", \"OptionsGurukul\", \"DarkHorseOfStockMarket\",\n",
    "        \"IntradayMatchSebiRegistered\", \"Sharekhan_Official\", \"CAJagdeesh\",\n",
    "        \"TheOriginalBull\", \"TheFinberg\", \"PowerOfStocks\", \"LiveTradingTricks\"\n",
    "    ]\n",
    "\n",
    "    all_data = []\n",
    "    for group in group_usernames:\n",
    "        try:\n",
    "            df_group = await scrape_group_messages(group, limit=200)\n",
    "            df_group[\"group\"] = group\n",
    "            all_data.append(df_group)\n",
    "            print(f\"‚úÖ Scraped {group}, messages: {len(df_group)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to fetch {group}: {e}\")\n",
    "\n",
    "    if all_data:\n",
    "        df_all = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "        # Save path\n",
    "        save_path = r\"D:\\Darryl\\Coding\\s_p\\data\\raw\"\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        csv_file = os.path.join(save_path, \"sebi_groups_messages.csv\")\n",
    "\n",
    "        df_all.to_csv(csv_file, index=False)\n",
    "        print(f\"üíæ Saved to {csv_file}\")\n",
    "\n",
    "        return df_all\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# ------------------------------\n",
    "# RUN\n",
    "# ------------------------------\n",
    "await client.start()\n",
    "df_all = await main()\n",
    "df_all.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3da6d44-90f6-4c55-b506-990619474b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "INPUT_PATH = r\"D:\\Darryl\\Coding\\s_p\\data\\raw\\sebi_groups_messages.csv\"\n",
    "OUTPUT_CSV = r\"D:\\Darryl\\Coding\\s_p\\data\\processed\\sebi_groups_messages_preprocessed.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "472ec74a-f780-43fb-a2e4-291b5839148d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re, html, unicodedata as ud\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Display options\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "pd.set_option(\"display.width\", 160)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6184a58f-4601-4d4c-a565-5a755a7ee487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5080, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_id</th>\n",
       "      <th>date</th>\n",
       "      <th>chat_id</th>\n",
       "      <th>sender_id</th>\n",
       "      <th>sender_username</th>\n",
       "      <th>sender_first_name</th>\n",
       "      <th>sender_last_name</th>\n",
       "      <th>sender_phone</th>\n",
       "      <th>text</th>\n",
       "      <th>views</th>\n",
       "      <th>forwards</th>\n",
       "      <th>reply_to_msg_id</th>\n",
       "      <th>media_type</th>\n",
       "      <th>has_hyperlink</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19552.0</td>\n",
       "      <td>2025-09-02 04:54:46+00:00</td>\n",
       "      <td>1.246175e+09</td>\n",
       "      <td>1.246175e+09</td>\n",
       "      <td>abhayvarn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47+ ü•≥üòçüíπü§©üòç\\nFirst Big Target done ü•≥üòçüíπ\\nMore than 90% Returns coming now ‚úåÔ∏èü•≥üí•üî•\\nStart Booking profits now and Trail SL STRICTLY ‚úåÔ∏èü§©üí•</td>\n",
       "      <td>3959.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19550.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>abhayvarn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19551.0</td>\n",
       "      <td>2025-09-02 04:31:22+00:00</td>\n",
       "      <td>1.246175e+09</td>\n",
       "      <td>1.246175e+09</td>\n",
       "      <td>abhayvarn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33+ ü•≥üòçüíπü§©üòç\\nMore than 40% Returns coming now ‚úåÔ∏èü•≥üí•üî•\\nStart Booking profits now and Trail SL STRICTLY ‚úåÔ∏èü§©üí•</td>\n",
       "      <td>4406.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19550.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>abhayvarn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19550.0</td>\n",
       "      <td>2025-09-02 04:24:35+00:00</td>\n",
       "      <td>1.246175e+09</td>\n",
       "      <td>1.246175e+09</td>\n",
       "      <td>abhayvarn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hero Zero \\n\\nBuy Nifty 24750 CE @ 23 - 28\\n\\nTARGET - 48 - 68 - 84 - 98\\n\\nSTOP LOSS  - 0</td>\n",
       "      <td>4656.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>abhayvarn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   message_id                       date       chat_id     sender_id sender_username  sender_first_name  sender_last_name  sender_phone  \\\n",
       "0     19552.0  2025-09-02 04:54:46+00:00  1.246175e+09  1.246175e+09       abhayvarn                NaN               NaN           NaN   \n",
       "1     19551.0  2025-09-02 04:31:22+00:00  1.246175e+09  1.246175e+09       abhayvarn                NaN               NaN           NaN   \n",
       "2     19550.0  2025-09-02 04:24:35+00:00  1.246175e+09  1.246175e+09       abhayvarn                NaN               NaN           NaN   \n",
       "\n",
       "                                                                                                                                 text   views  forwards  \\\n",
       "0  47+ ü•≥üòçüíπü§©üòç\\nFirst Big Target done ü•≥üòçüíπ\\nMore than 90% Returns coming now ‚úåÔ∏èü•≥üí•üî•\\nStart Booking profits now and Trail SL STRICTLY ‚úåÔ∏èü§©üí•  3959.0       0.0   \n",
       "1                             33+ ü•≥üòçüíπü§©üòç\\nMore than 40% Returns coming now ‚úåÔ∏èü•≥üí•üî•\\nStart Booking profits now and Trail SL STRICTLY ‚úåÔ∏èü§©üí•  4406.0       0.0   \n",
       "2                                          Hero Zero \\n\\nBuy Nifty 24750 CE @ 23 - 28\\n\\nTARGET - 48 - 68 - 84 - 98\\n\\nSTOP LOSS  - 0  4656.0       3.0   \n",
       "\n",
       "   reply_to_msg_id media_type  has_hyperlink      group  \n",
       "0          19550.0        NaN          False  abhayvarn  \n",
       "1          19550.0        NaN          False  abhayvarn  \n",
       "2              NaN        NaN          False  abhayvarn  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- CSV loader with encoding fallbacks ----\n",
    "def load_csv_safely(path):\n",
    "    encodings = [\"utf-8\", \"utf-8-sig\", \"latin1\"]\n",
    "    errors = []\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, low_memory=False)\n",
    "        except Exception as e:\n",
    "            errors.append((enc, str(e)))\n",
    "    try:\n",
    "        return pd.read_csv(path, engine=\"python\", low_memory=False)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to read CSV with tried encodings: {errors} and python engine: {e}\")\n",
    "\n",
    "df = load_csv_safely(INPUT_PATH)\n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fc15055-74f7-4468-874a-54c2567ef62a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_col': 'text',\n",
       " 'date_col': 'date',\n",
       " 'sender_col': None,\n",
       " 'message_id_col': 'message_id',\n",
       " 'reply_to_col': None,\n",
       " 'forward_from_col': None,\n",
       " 'views_col': 'views',\n",
       " 'reactions_col': None,\n",
       " 'channel_col': 'group'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- Guess likely column names ----\n",
    "def choose_col(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "text_col = choose_col(df, [\"text\", \"message\", \"content\", \"body\"])\n",
    "date_col = choose_col(df, [\"date\", \"datetime\", \"timestamp\", \"time\"])\n",
    "sender_col = choose_col(df, [\"from\", \"from_name\", \"sender\", \"author\", \"user\", \"username\", \"name\"])\n",
    "message_id_col = choose_col(df, [\"id\", \"message_id\", \"msg_id\"])\n",
    "reply_to_col = choose_col(df, [\"reply_to_message_id\", \"reply_to\", \"in_reply_to\"])\n",
    "forward_from_col = choose_col(df, [\"forwarded_from\", \"forward_from\", \"fwd_from\"])\n",
    "views_col = choose_col(df, [\"views\", \"view_count\"])\n",
    "reactions_col = choose_col(df, [\"reactions\", \"reaction_count\"])\n",
    "channel_col = choose_col(df, [\"chat\", \"chat_name\", \"channel\", \"group\", \"group_name\", \"channel_title\"])\n",
    "\n",
    "guessed = {\n",
    "    \"text_col\": text_col,\n",
    "    \"date_col\": date_col,\n",
    "    \"sender_col\": sender_col,\n",
    "    \"message_id_col\": message_id_col,\n",
    "    \"reply_to_col\": reply_to_col,\n",
    "    \"forward_from_col\": forward_from_col,\n",
    "    \"views_col\": views_col,\n",
    "    \"reactions_col\": reactions_col,\n",
    "    \"channel_col\": channel_col,\n",
    "}\n",
    "guessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0be80c4-3315-45b0-bdbf-d53d60c68ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Regexes & utils ----\n",
    "URL_RE = re.compile(r\"(https?://\\S+|www\\.\\S+)\", flags=re.IGNORECASE)\n",
    "MENTION_RE = re.compile(r\"@[\\w_]{3,50}\")\n",
    "HASHTAG_RE = re.compile(r\"(?:^|\\s)#(\\w{2,})\")\n",
    "CASHTAG_RE = re.compile(r\"(?:^|\\s)\\$([A-Za-z]{1,10})\")\n",
    "EMAIL_RE = re.compile(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\")\n",
    "PHONE_RE = re.compile(r\"(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\(?\\d{2,4}\\)?[-.\\s]?){2,4}\\d{3,4}\")\n",
    "EMOJI_RE = re.compile(\"[\"\n",
    "    \"\\U0001F1E0-\\U0001F1FF\"\n",
    "    \"\\U0001F300-\\U0001F5FF\"\n",
    "    \"\\U0001F600-\\U0001F64F\"\n",
    "    \"\\U0001F680-\\U0001F6FF\"\n",
    "    \"\\U0001F700-\\U0001F77F\"\n",
    "    \"\\U0001F780-\\U0001F7FF\"\n",
    "    \"\\U0001F800-\\U0001F8FF\"\n",
    "    \"\\U0001F900-\\U0001F9FF\"\n",
    "    \"\\U0001FA00-\\U0001FA6F\"\n",
    "    \"\\U0001FA70-\\U0001FAFF\"\n",
    "    \"\\u2700-\\u27BF\"\n",
    "    \"\\u2600-\\u26FF\"\n",
    "\"]+\", flags=re.UNICODE)\n",
    "\n",
    "ZW_RE = re.compile(r\"[\\u200B-\\u200D\\uFEFF]\")\n",
    "CTRL_RE = re.compile(r\"[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F]\")\n",
    "\n",
    "def to_nfc(s: str) -> str:\n",
    "    return ud.normalize(\"NFC\", s)\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        s = \"\" if pd.isna(s) else str(s)\n",
    "    s = html.unescape(s)\n",
    "    s = to_nfc(s)\n",
    "    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    s = ZW_RE.sub(\"\", s)\n",
    "    s = CTRL_RE.sub(\"\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "def extract_list(pattern, s: str):\n",
    "    if not isinstance(s, str):\n",
    "        return []\n",
    "    return [m.group(0) for m in pattern.finditer(s)]\n",
    "\n",
    "def extract_group(pattern, s: str, group_idx=1):\n",
    "    if not isinstance(s, str):\n",
    "        return []\n",
    "    return [m.group(group_idx) for m in pattern.finditer(s)]\n",
    "\n",
    "SYSTEM_LIKE = re.compile(\n",
    "    r\"^(joined the group|left the group|pinned a message|changed the group|added .* to the group)\",\n",
    "    flags=re.IGNORECASE,\n",
    ")\n",
    "\n",
    "def looks_like_system_msg(s: str) -> bool:\n",
    "    if not isinstance(s, str):\n",
    "        return False\n",
    "    return bool(SYSTEM_LIKE.search(s.strip()))\n",
    "\n",
    "def tokenise_basic(s: str) -> list:\n",
    "    if not isinstance(s, str):\n",
    "        return []\n",
    "    return re.findall(r\"[^\\W_][\\w‚Äô'-]*\", s, flags=re.UNICODE)\n",
    "\n",
    "def clean_for_nlp(s: str, remove_urls=True, remove_emojis=True):\n",
    "    s = normalize_text(s)\n",
    "    if remove_urls:\n",
    "        s = URL_RE.sub(\" \", s)\n",
    "    if remove_emojis:\n",
    "        s = EMOJI_RE.sub(\" \", s)\n",
    "    s = re.sub(r\"[^\\w\\s@#\\$‚Äô'-]\", \" \", s, flags=re.UNICODE)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def safe_to_datetime(series, assumed_tz=\"UTC\", target_tz=\"Asia/Kolkata\"):\n",
    "    dt = pd.to_datetime(series, errors=\"coerce\", utc=True)\n",
    "    try:\n",
    "        dt = dt.dt.tz_convert(ZoneInfo(target_tz))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0802816b-c213-4d90-9dfc-bb332af3c22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped duplicates: 296 based on keys: ['channel_norm', 'text_norm', 'date_parsed_ist']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_id</th>\n",
       "      <th>date</th>\n",
       "      <th>chat_id</th>\n",
       "      <th>sender_id</th>\n",
       "      <th>sender_username</th>\n",
       "      <th>sender_first_name</th>\n",
       "      <th>sender_last_name</th>\n",
       "      <th>sender_phone</th>\n",
       "      <th>text</th>\n",
       "      <th>views</th>\n",
       "      <th>...</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>message_id_norm</th>\n",
       "      <th>channel_norm</th>\n",
       "      <th>views_norm</th>\n",
       "      <th>date_parsed_ist</th>\n",
       "      <th>time</th>\n",
       "      <th>hour</th>\n",
       "      <th>dow</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19552.0</td>\n",
       "      <td>2025-09-02</td>\n",
       "      <td>1.246175e+09</td>\n",
       "      <td>1.246175e+09</td>\n",
       "      <td>abhayvarn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47+ ü•≥üòçüíπü§©üòç\\nFirst Big Target done ü•≥üòçüíπ\\nMore than 90% Returns coming now ‚úåÔ∏èü•≥üí•üî•\\nStart Booking profits now and Trail SL STRICTLY ‚úåÔ∏èü§©üí•</td>\n",
       "      <td>3959.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>19552.0</td>\n",
       "      <td>abhayvarn</td>\n",
       "      <td>3959.0</td>\n",
       "      <td>2025-09-02 10:24:46+05:30</td>\n",
       "      <td>10:24:46</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19551.0</td>\n",
       "      <td>2025-09-02</td>\n",
       "      <td>1.246175e+09</td>\n",
       "      <td>1.246175e+09</td>\n",
       "      <td>abhayvarn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33+ ü•≥üòçüíπü§©üòç\\nMore than 40% Returns coming now ‚úåÔ∏èü•≥üí•üî•\\nStart Booking profits now and Trail SL STRICTLY ‚úåÔ∏èü§©üí•</td>\n",
       "      <td>4406.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>19551.0</td>\n",
       "      <td>abhayvarn</td>\n",
       "      <td>4406.0</td>\n",
       "      <td>2025-09-02 10:01:22+05:30</td>\n",
       "      <td>10:01:22</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19550.0</td>\n",
       "      <td>2025-09-02</td>\n",
       "      <td>1.246175e+09</td>\n",
       "      <td>1.246175e+09</td>\n",
       "      <td>abhayvarn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hero Zero \\n\\nBuy Nifty 24750 CE @ 23 - 28\\n\\nTARGET - 48 - 68 - 84 - 98\\n\\nSTOP LOSS  - 0</td>\n",
       "      <td>4656.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>19550.0</td>\n",
       "      <td>abhayvarn</td>\n",
       "      <td>4656.0</td>\n",
       "      <td>2025-09-02 09:54:35+05:30</td>\n",
       "      <td>09:54:35</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19548.0</td>\n",
       "      <td>2025-09-02</td>\n",
       "      <td>1.246175e+09</td>\n",
       "      <td>1.246175e+09</td>\n",
       "      <td>abhayvarn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.youtube.com/live/SUodqXwmxMs?si=Ho4TiQ9RXBsdxxgN</td>\n",
       "      <td>4830.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>19548.0</td>\n",
       "      <td>abhayvarn</td>\n",
       "      <td>4830.0</td>\n",
       "      <td>2025-09-02 09:37:17+05:30</td>\n",
       "      <td>09:37:17</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19547.0</td>\n",
       "      <td>2025-09-02</td>\n",
       "      <td>1.246175e+09</td>\n",
       "      <td>1.246175e+09</td>\n",
       "      <td>abhayvarn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Good Newzz before Nifty Weekly Expiry ü§©üí•üî•\\n\\nOption Trading Premium Channel Joining Link is active for 10 Members to get the feedback ‚úÖÔ∏èüòâ\\n\\nhttps://tinyurl.com/22usq7jn</td>\n",
       "      <td>4974.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>19547.0</td>\n",
       "      <td>abhayvarn</td>\n",
       "      <td>4974.0</td>\n",
       "      <td>2025-09-02 07:57:26+05:30</td>\n",
       "      <td>07:57:26</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   message_id        date       chat_id     sender_id sender_username  sender_first_name  sender_last_name  sender_phone  \\\n",
       "0     19552.0  2025-09-02  1.246175e+09  1.246175e+09       abhayvarn                NaN               NaN           NaN   \n",
       "1     19551.0  2025-09-02  1.246175e+09  1.246175e+09       abhayvarn                NaN               NaN           NaN   \n",
       "2     19550.0  2025-09-02  1.246175e+09  1.246175e+09       abhayvarn                NaN               NaN           NaN   \n",
       "3     19548.0  2025-09-02  1.246175e+09  1.246175e+09       abhayvarn                NaN               NaN           NaN   \n",
       "4     19547.0  2025-09-02  1.246175e+09  1.246175e+09       abhayvarn                NaN               NaN           NaN   \n",
       "\n",
       "                                                                                                                                                                        text  \\\n",
       "0                                         47+ ü•≥üòçüíπü§©üòç\\nFirst Big Target done ü•≥üòçüíπ\\nMore than 90% Returns coming now ‚úåÔ∏èü•≥üí•üî•\\nStart Booking profits now and Trail SL STRICTLY ‚úåÔ∏èü§©üí•   \n",
       "1                                                                    33+ ü•≥üòçüíπü§©üòç\\nMore than 40% Returns coming now ‚úåÔ∏èü•≥üí•üî•\\nStart Booking profits now and Trail SL STRICTLY ‚úåÔ∏èü§©üí•   \n",
       "2                                                                                 Hero Zero \\n\\nBuy Nifty 24750 CE @ 23 - 28\\n\\nTARGET - 48 - 68 - 84 - 98\\n\\nSTOP LOSS  - 0   \n",
       "3                                                                                                               https://www.youtube.com/live/SUodqXwmxMs?si=Ho4TiQ9RXBsdxxgN   \n",
       "4  Good Newzz before Nifty Weekly Expiry ü§©üí•üî•\\n\\nOption Trading Premium Channel Joining Link is active for 10 Members to get the feedback ‚úÖÔ∏èüòâ\\n\\nhttps://tinyurl.com/22usq7jn   \n",
       "\n",
       "    views  ...  emoji_count  message_id_norm channel_norm  views_norm           date_parsed_ist      time hour dow month  year  \n",
       "0  3959.0  ...            6          19552.0    abhayvarn      3959.0 2025-09-02 10:24:46+05:30  10:24:46   10   1     9  2025  \n",
       "1  4406.0  ...            5          19551.0    abhayvarn      4406.0 2025-09-02 10:01:22+05:30  10:01:22   10   1     9  2025  \n",
       "2  4656.0  ...            0          19550.0    abhayvarn      4656.0 2025-09-02 09:54:35+05:30  09:54:35    9   1     9  2025  \n",
       "3  4830.0  ...            0          19548.0    abhayvarn      4830.0 2025-09-02 09:37:17+05:30  09:37:17    9   1     9  2025  \n",
       "4  4974.0  ...            3          19547.0    abhayvarn      4974.0 2025-09-02 07:57:26+05:30  07:57:26    7   1     9  2025  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- Apply preprocessing ----\n",
    "df_proc = df.copy()\n",
    "\n",
    "if text_col is not None:\n",
    "    df_proc[\"text_raw\"] = df_proc[text_col].astype(str)\n",
    "    df_proc[\"text_norm\"] = df_proc[\"text_raw\"].map(normalize_text)\n",
    "    df_proc[\"text_clean\"] = df_proc[\"text_norm\"].map(lambda s: clean_for_nlp(s, remove_urls=False, remove_emojis=False))\n",
    "    df_proc[\"urls\"] = df_proc[\"text_norm\"].map(lambda s: extract_list(URL_RE, s))\n",
    "    df_proc[\"mentions\"] = df_proc[\"text_norm\"].map(lambda s: extract_list(MENTION_RE, s))\n",
    "    df_proc[\"hashtags\"] = df_proc[\"text_norm\"].map(lambda s: extract_group(HASHTAG_RE, s))\n",
    "    df_proc[\"cashtags\"] = df_proc[\"text_norm\"].map(lambda s: extract_group(CASHTAG_RE, s))\n",
    "    df_proc[\"emails\"] = df_proc[\"text_norm\"].map(lambda s: extract_list(EMAIL_RE, s))\n",
    "    df_proc[\"phones_maybe\"] = df_proc[\"text_norm\"].map(lambda s: extract_list(PHONE_RE, s))\n",
    "    df_proc[\"emojis\"] = df_proc[\"text_norm\"].map(lambda s: extract_list(EMOJI_RE, s))\n",
    "\n",
    "    df_proc[\"is_system_like\"] = df_proc[\"text_norm\"].map(looks_like_system_msg)\n",
    "    df_proc[\"char_count\"] = df_proc[\"text_norm\"].map(lambda s: len(s) if isinstance(s, str) else 0)\n",
    "    df_proc[\"word_count\"] = df_proc[\"text_norm\"].map(lambda s: len(tokenise_basic(s)))\n",
    "    df_proc[\"url_count\"] = df_proc[\"urls\"].map(len)\n",
    "    df_proc[\"mention_count\"] = df_proc[\"mentions\"].map(len)\n",
    "    df_proc[\"hashtag_count\"] = df_proc[\"hashtags\"].map(len)\n",
    "    df_proc[\"cashtag_count\"] = df_proc[\"cashtags\"].map(len)\n",
    "    df_proc[\"email_count\"] = df_proc[\"emails\"].map(len)\n",
    "    df_proc[\"emoji_count\"] = df_proc[\"emojis\"].map(len)\n",
    "\n",
    "if sender_col is not None:\n",
    "    df_proc[\"sender_raw\"] = df_proc[sender_col].astype(str)\n",
    "    df_proc[\"sender_norm\"] = df_proc[\"sender_raw\"].map(normalize_text).replace({\"nan\": np.nan})\n",
    "\n",
    "if message_id_col is not None:\n",
    "    df_proc[\"message_id_norm\"] = df_proc[message_id_col]\n",
    "\n",
    "if reply_to_col is not None:\n",
    "    df_proc[\"reply_to_norm\"] = df_proc[reply_to_col]\n",
    "\n",
    "if forward_from_col is not None:\n",
    "    df_proc[\"forward_from_norm\"] = df_proc[forward_from_col].astype(str).replace({\"nan\": np.nan})\n",
    "\n",
    "if channel_col is not None:\n",
    "    df_proc[\"channel_norm\"] = df_proc[channel_col].astype(str).map(normalize_text).replace({\"nan\": np.nan})\n",
    "\n",
    "if views_col is not None and views_col in df_proc.columns:\n",
    "    df_proc[\"views_norm\"] = pd.to_numeric(df_proc[views_col], errors=\"coerce\")\n",
    "\n",
    "if reactions_col is not None and reactions_col in df_proc.columns:\n",
    "    df_proc[\"reactions_norm\"] = pd.to_numeric(df_proc[reactions_col], errors=\"coerce\")\n",
    "\n",
    "if date_col is not None:\n",
    "    df_proc[\"date_parsed_ist\"] = safe_to_datetime(df_proc[date_col], target_tz=\"Asia/Kolkata\")\n",
    "    dt = df_proc[\"date_parsed_ist\"]\n",
    "    df_proc[\"date\"] = dt.dt.date\n",
    "    df_proc[\"time\"] = dt.dt.time\n",
    "    df_proc[\"hour\"] = dt.dt.hour\n",
    "    df_proc[\"dow\"] = dt.dt.dayofweek\n",
    "    df_proc[\"month\"] = dt.dt.month\n",
    "    df_proc[\"year\"] = dt.dt.year\n",
    "\n",
    "# De-duplication\n",
    "dedupe_keys = [c for c in [\"channel_norm\", \"sender_norm\", \"text_norm\", \"date_parsed_ist\"] if c in df_proc.columns]\n",
    "if dedupe_keys:\n",
    "    before = len(df_proc)\n",
    "    df_proc = df_proc.drop_duplicates(subset=dedupe_keys, keep=\"first\").reset_index(drop=True)\n",
    "    print(f\"Dropped duplicates: {before - len(df_proc)} based on keys: {dedupe_keys}\")\n",
    "\n",
    "df_proc.to_csv(OUTPUT_CSV, index=False)\n",
    "df_proc.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c99fb759-2a35-4248-9861-95e53536ef10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded shape: (5080, 15)\n",
      "Dropped duplicates: 296 based on ['channel_norm', 'text_norm', 'date_parsed_ist']\n",
      "After fuzzy dedupe rows: 4098\n",
      "Saved final preprocessed: D:\\Darryl\\Coding\\s_p\\data\\processed\\sebi_groups_messages_preprocessed_final.csv\n",
      "Saved promo sample: D:\\Darryl\\Coding\\s_p\\data\\processed\\promo_sample_for_labeling.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_id</th>\n",
       "      <th>date</th>\n",
       "      <th>chat_id</th>\n",
       "      <th>sender_id</th>\n",
       "      <th>sender_username</th>\n",
       "      <th>sender_first_name</th>\n",
       "      <th>sender_last_name</th>\n",
       "      <th>sender_phone</th>\n",
       "      <th>text</th>\n",
       "      <th>views</th>\n",
       "      <th>...</th>\n",
       "      <th>views_bucket</th>\n",
       "      <th>date_parsed_ist</th>\n",
       "      <th>hour</th>\n",
       "      <th>dow</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>trade_action</th>\n",
       "      <th>trade_strikes</th>\n",
       "      <th>trade_targets</th>\n",
       "      <th>trade_stoploss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2850</th>\n",
       "      <td>29017.0</td>\n",
       "      <td>2025-06-23</td>\n",
       "      <td>1.354171e+09</td>\n",
       "      <td>1.354171e+09</td>\n",
       "      <td>AngelOneAdvisory</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>üü¢ **BUY** POONAWALLA **1** shares at **435.70**.\\n\\n__Message : SL 422 TGT 457 Modify Qty/ Lot as per your discretion__\\n\\n__Created Date &amp; Time\\n03:06 PM__\\n__23/06/25__\\n\\n__Disclaimer : __[__ w...</td>\n",
       "      <td>32312.0</td>\n",
       "      <td>...</td>\n",
       "      <td>&gt;10k</td>\n",
       "      <td>2025-06-23 15:06:38+05:30</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2025</td>\n",
       "      <td>buy</td>\n",
       "      <td>435,422,457</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2851</th>\n",
       "      <td>29016.0</td>\n",
       "      <td>2025-06-23</td>\n",
       "      <td>1.354171e+09</td>\n",
       "      <td>1.354171e+09</td>\n",
       "      <td>AngelOneAdvisory</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>üü¢ **BUY** DMART **1** shares at **4337.00**.\\n\\n__Message : SL 4260 TGT 4460 Modify Qty/ Lot as per your discretion__\\n\\n__Created Date &amp; Time\\n02:48 PM__\\n__23/06/25__\\n\\n__Disclaimer : __[__ www...</td>\n",
       "      <td>29776.0</td>\n",
       "      <td>...</td>\n",
       "      <td>&gt;10k</td>\n",
       "      <td>2025-06-23 14:48:37+05:30</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2025</td>\n",
       "      <td>buy</td>\n",
       "      <td>4337,4260,4460</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2852</th>\n",
       "      <td>29015.0</td>\n",
       "      <td>2025-06-23</td>\n",
       "      <td>1.354171e+09</td>\n",
       "      <td>1.354171e+09</td>\n",
       "      <td>AngelOneAdvisory</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BOOK PROFIT IN NIFTY 24900 PE@100.2\\n\\n__Created Date &amp; Time__\\n__02:31 PM__\\n__23/06/25__</td>\n",
       "      <td>29378.0</td>\n",
       "      <td>...</td>\n",
       "      <td>&gt;10k</td>\n",
       "      <td>2025-06-23 14:31:42+05:30</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2025</td>\n",
       "      <td>None</td>\n",
       "      <td>24900,100</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2853</th>\n",
       "      <td>29014.0</td>\n",
       "      <td>2025-06-23</td>\n",
       "      <td>1.354171e+09</td>\n",
       "      <td>1.354171e+09</td>\n",
       "      <td>AngelOneAdvisory</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EXIT M&amp;M 3200CE AT 12.20\\n\\n__Created Date &amp; Time__\\n__02:30 PM__\\n__23/06/25__</td>\n",
       "      <td>27925.0</td>\n",
       "      <td>...</td>\n",
       "      <td>&gt;10k</td>\n",
       "      <td>2025-06-23 14:30:33+05:30</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2025</td>\n",
       "      <td>None</td>\n",
       "      <td>3200</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2854</th>\n",
       "      <td>29013.0</td>\n",
       "      <td>2025-06-23</td>\n",
       "      <td>1.354171e+09</td>\n",
       "      <td>1.354171e+09</td>\n",
       "      <td>AngelOneAdvisory</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BOOK PROFIT IN EICHERMOT @ 5552\\n\\n__Created Date &amp; Time__\\n__02:22 PM__\\n__23/06/25__</td>\n",
       "      <td>26807.0</td>\n",
       "      <td>...</td>\n",
       "      <td>&gt;10k</td>\n",
       "      <td>2025-06-23 14:22:09+05:30</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2025</td>\n",
       "      <td>None</td>\n",
       "      <td>5552</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      message_id        date       chat_id     sender_id   sender_username  sender_first_name  sender_last_name  sender_phone  \\\n",
       "2850     29017.0  2025-06-23  1.354171e+09  1.354171e+09  AngelOneAdvisory                NaN               NaN           NaN   \n",
       "2851     29016.0  2025-06-23  1.354171e+09  1.354171e+09  AngelOneAdvisory                NaN               NaN           NaN   \n",
       "2852     29015.0  2025-06-23  1.354171e+09  1.354171e+09  AngelOneAdvisory                NaN               NaN           NaN   \n",
       "2853     29014.0  2025-06-23  1.354171e+09  1.354171e+09  AngelOneAdvisory                NaN               NaN           NaN   \n",
       "2854     29013.0  2025-06-23  1.354171e+09  1.354171e+09  AngelOneAdvisory                NaN               NaN           NaN   \n",
       "\n",
       "                                                                                                                                                                                                         text  \\\n",
       "2850  üü¢ **BUY** POONAWALLA **1** shares at **435.70**.\\n\\n__Message : SL 422 TGT 457 Modify Qty/ Lot as per your discretion__\\n\\n__Created Date & Time\\n03:06 PM__\\n__23/06/25__\\n\\n__Disclaimer : __[__ w...   \n",
       "2851  üü¢ **BUY** DMART **1** shares at **4337.00**.\\n\\n__Message : SL 4260 TGT 4460 Modify Qty/ Lot as per your discretion__\\n\\n__Created Date & Time\\n02:48 PM__\\n__23/06/25__\\n\\n__Disclaimer : __[__ www...   \n",
       "2852                                                                                                               BOOK PROFIT IN NIFTY 24900 PE@100.2\\n\\n__Created Date & Time__\\n__02:31 PM__\\n__23/06/25__   \n",
       "2853                                                                                                                          EXIT M&M 3200CE AT 12.20\\n\\n__Created Date & Time__\\n__02:30 PM__\\n__23/06/25__   \n",
       "2854                                                                                                                   BOOK PROFIT IN EICHERMOT @ 5552\\n\\n__Created Date & Time__\\n__02:22 PM__\\n__23/06/25__   \n",
       "\n",
       "        views  ...  views_bucket           date_parsed_ist hour  dow month  year trade_action   trade_strikes trade_targets trade_stoploss  \n",
       "2850  32312.0  ...          >10k 2025-06-23 15:06:38+05:30   15    0     6  2025          buy     435,422,457                               \n",
       "2851  29776.0  ...          >10k 2025-06-23 14:48:37+05:30   14    0     6  2025          buy  4337,4260,4460                               \n",
       "2852  29378.0  ...          >10k 2025-06-23 14:31:42+05:30   14    0     6  2025         None       24900,100                               \n",
       "2853  27925.0  ...          >10k 2025-06-23 14:30:33+05:30   14    0     6  2025         None            3200                               \n",
       "2854  26807.0  ...          >10k 2025-06-23 14:22:09+05:30   14    0     6  2025         None            5552                               \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Paths ===\n",
    "INPUT_PATH = r\"D:\\Darryl\\Coding\\s_p\\data\\raw\\sebi_groups_messages.csv\"\n",
    "OUTPUT_CSV = r\"D:\\Darryl\\Coding\\s_p\\data\\processed\\sebi_groups_messages_preprocessed_final.csv\"\n",
    "PROMO_SAMPLE_CSV = r\"D:\\Darryl\\Coding\\s_p\\data\\processed\\promo_sample_for_labeling.csv\"\n",
    "\n",
    "# === Imports ===\n",
    "import re, html, unicodedata as ud\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# sklearn only needed if fuzzy dedupe is enabled\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Display options\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "# ---- CSV loader with encoding fallbacks ----\n",
    "def load_csv_safely(path):\n",
    "    encodings = [\"utf-8\", \"utf-8-sig\", \"latin1\"]\n",
    "    errors = []\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, low_memory=False)\n",
    "        except Exception as e:\n",
    "            errors.append((enc, str(e)))\n",
    "    try:\n",
    "        return pd.read_csv(path, engine=\"python\", low_memory=False)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to read CSV with tried encodings: {errors} and python engine: {e}\")\n",
    "\n",
    "df = load_csv_safely(INPUT_PATH)\n",
    "print(\"Loaded shape:\", df.shape)\n",
    "\n",
    "# ---- Guess likely column names ----\n",
    "def choose_col(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "text_col = choose_col(df, [\"text\", \"message\", \"content\", \"body\"])\n",
    "date_col = choose_col(df, [\"date\", \"datetime\", \"timestamp\", \"time\"])\n",
    "sender_col = choose_col(df, [\"from\", \"from_name\", \"sender\", \"author\", \"user\", \"username\", \"name\"])\n",
    "message_id_col = choose_col(df, [\"id\", \"message_id\", \"msg_id\"])\n",
    "reply_to_col = choose_col(df, [\"reply_to_message_id\", \"reply_to\", \"in_reply_to\"])\n",
    "forward_from_col = choose_col(df, [\"forwarded_from\", \"forward_from\", \"fwd_from\"])\n",
    "views_col = choose_col(df, [\"views\", \"view_count\"])\n",
    "reactions_col = choose_col(df, [\"reactions\", \"reaction_count\"])\n",
    "channel_col = choose_col(df, [\"chat\", \"chat_name\", \"channel\", \"group\", \"group_name\", \"channel_title\"])\n",
    "\n",
    "# ---- Regexes & utils ----\n",
    "URL_RE = re.compile(r\"(https?://\\S+|www\\.\\S+)\", flags=re.IGNORECASE)\n",
    "MENTION_RE = re.compile(r\"@[\\w_]{3,50}\")\n",
    "HASHTAG_RE = re.compile(r\"(?:^|\\s)#(\\w{2,})\")\n",
    "CASHTAG_RE = re.compile(r\"(?:^|\\s)\\$([A-Za-z]{1,10})\")\n",
    "EMAIL_RE = re.compile(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\")\n",
    "PHONE_RE = re.compile(r\"(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\(?\\d{2,4}\\)?[-.\\s]?){2,4}\\d{3,4}\")\n",
    "EMOJI_RE = re.compile(\"[\"\n",
    "    \"\\U0001F1E0-\\U0001F1FF\"\n",
    "    \"\\U0001F300-\\U0001F5FF\"\n",
    "    \"\\U0001F600-\\U0001F64F\"\n",
    "    \"\\U0001F680-\\U0001F6FF\"\n",
    "    \"\\U0001F700-\\U0001F77F\"\n",
    "    \"\\U0001F780-\\U0001F7FF\"\n",
    "    \"\\U0001F800-\\U0001F8FF\"\n",
    "    \"\\U0001F900-\\U0001F9FF\"\n",
    "    \"\\U0001FA00-\\U0001FA6F\"\n",
    "    \"\\U0001FA70-\\U0001FAFF\"\n",
    "    \"\\u2700-\\u27BF\"\n",
    "    \"\\u2600-\\u26FF\"\n",
    "\"]+\", flags=re.UNICODE)\n",
    "\n",
    "ZW_RE = re.compile(r\"[\\u200B-\\u200D\\uFEFF]\")\n",
    "CTRL_RE = re.compile(r\"[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F]\")\n",
    "\n",
    "SYSTEM_LIKE = re.compile(\n",
    "    r\"^(joined the group|left the group|pinned a message|changed the group|added .* to the group)\",\n",
    "    flags=re.IGNORECASE,\n",
    ")\n",
    "\n",
    "def to_nfc(s: str) -> str:\n",
    "    return ud.normalize(\"NFC\", s)\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        s = \"\" if pd.isna(s) else str(s)\n",
    "    s = html.unescape(s)\n",
    "    s = to_nfc(s)\n",
    "    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    s = ZW_RE.sub(\"\", s)\n",
    "    s = CTRL_RE.sub(\"\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "def extract_list(pattern, s: str):\n",
    "    if not isinstance(s, str):\n",
    "        return []\n",
    "    return [m.group(0) for m in pattern.finditer(s)]\n",
    "\n",
    "def extract_group(pattern, s: str, group_idx=1):\n",
    "    if not isinstance(s, str):\n",
    "        return []\n",
    "    return [m.group(group_idx) for m in pattern.finditer(s)]\n",
    "\n",
    "def looks_like_system_msg(s: str) -> bool:\n",
    "    return bool(SYSTEM_LIKE.search(s.strip())) if isinstance(s, str) else False\n",
    "\n",
    "def tokenise_basic(s: str) -> list:\n",
    "    return re.findall(r\"[^\\W_][\\w‚Äô'-]*\", s, flags=re.UNICODE) if isinstance(s, str) else []\n",
    "\n",
    "def clean_for_nlp(s: str, remove_urls=True, remove_emojis=True):\n",
    "    s = normalize_text(s)\n",
    "    if remove_urls:\n",
    "        s = URL_RE.sub(\" \", s)\n",
    "    if remove_emojis:\n",
    "        s = EMOJI_RE.sub(\" \", s)\n",
    "    s = re.sub(r\"[^\\w\\s@#\\$‚Äô'-]\", \" \", s, flags=re.UNICODE)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def safe_to_datetime(series, target_tz=\"Asia/Kolkata\"):\n",
    "    dt = pd.to_datetime(series, errors=\"coerce\", utc=True)\n",
    "    try:\n",
    "        dt = dt.dt.tz_convert(ZoneInfo(target_tz))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return dt\n",
    "\n",
    "# PII redaction\n",
    "def redact_text(s):\n",
    "    if not isinstance(s, str): return s\n",
    "    s = EMAIL_RE.sub(\"<EMAIL>\", s)\n",
    "    s = PHONE_RE.sub(\"<PHONE>\", s)\n",
    "    return s\n",
    "\n",
    "def mask_username(u):\n",
    "    if not isinstance(u, str) or u.strip()==\"\":\n",
    "        return u\n",
    "    u = u.strip()\n",
    "    if len(u) <= 2: return u[0] + \"*\"\n",
    "    return u[0] + \"*\"*(min(6,len(u)-2)) + u[-1]\n",
    "\n",
    "# ---- Apply preprocessing ----\n",
    "df_proc = df.copy()\n",
    "\n",
    "if text_col:\n",
    "    df_proc[\"text_raw\"] = df_proc[text_col].astype(str)\n",
    "    df_proc[\"text_norm\"] = df_proc[\"text_raw\"].map(normalize_text)\n",
    "    df_proc[\"text_clean\"] = df_proc[\"text_norm\"].map(lambda s: clean_for_nlp(s, remove_urls=False, remove_emojis=False))\n",
    "    df_proc[\"text_redacted\"] = df_proc[\"text_norm\"].map(redact_text)\n",
    "    df_proc[\"text_for_model\"] = df_proc[\"text_redacted\"].map(lambda s: clean_for_nlp(s, remove_urls=True, remove_emojis=True).lower())\n",
    "    df_proc[\"urls\"] = df_proc[\"text_norm\"].map(lambda s: extract_list(URL_RE, s))\n",
    "    df_proc[\"mentions\"] = df_proc[\"text_norm\"].map(lambda s: extract_list(MENTION_RE, s))\n",
    "    df_proc[\"hashtags\"] = df_proc[\"text_norm\"].map(lambda s: extract_group(HASHTAG_RE, s))\n",
    "    df_proc[\"cashtags\"] = df_proc[\"text_norm\"].map(lambda s: extract_group(CASHTAG_RE, s))\n",
    "    df_proc[\"emails\"] = df_proc[\"text_norm\"].map(lambda s: extract_list(EMAIL_RE, s))\n",
    "    df_proc[\"phones_maybe\"] = df_proc[\"text_norm\"].map(lambda s: extract_list(PHONE_RE, s))\n",
    "    df_proc[\"emojis\"] = df_proc[\"text_norm\"].map(lambda s: extract_list(EMOJI_RE, s))\n",
    "    df_proc[\"is_system_like\"] = df_proc[\"text_norm\"].map(looks_like_system_msg)\n",
    "    df_proc[\"char_count\"] = df_proc[\"text_norm\"].map(lambda s: len(s) if isinstance(s, str) else 0)\n",
    "    df_proc[\"word_count\"] = df_proc[\"text_norm\"].map(lambda s: len(tokenise_basic(s)))\n",
    "    df_proc[\"emoji_count\"] = df_proc[\"emojis\"].map(len)\n",
    "    # promo-like flag\n",
    "    PROMO_RE = re.compile(r\"(join|subscribe|offer|http|www\\.|t\\.me|channel|dm|discount)\", flags=re.I)\n",
    "    df_proc[\"is_promo\"] = df_proc[\"text_norm\"].map(lambda s: bool(PROMO_RE.search(s)) if isinstance(s,str) else False)\n",
    "\n",
    "if sender_col:\n",
    "    df_proc[\"sender_raw\"] = df_proc[sender_col].astype(str)\n",
    "    df_proc[\"sender_norm\"] = df_proc[\"sender_raw\"].map(normalize_text).replace({\"nan\": np.nan})\n",
    "    df_proc[\"sender_username_masked\"] = df_proc[\"sender_norm\"].map(mask_username)\n",
    "\n",
    "if message_id_col:\n",
    "    df_proc[\"message_id_norm\"] = df_proc[message_id_col]\n",
    "\n",
    "if reply_to_col:\n",
    "    df_proc[\"reply_to_norm\"] = df_proc[reply_to_col]\n",
    "\n",
    "if forward_from_col:\n",
    "    df_proc[\"forward_from_norm\"] = df_proc[forward_from_col].astype(str).replace({\"nan\": np.nan})\n",
    "\n",
    "if channel_col:\n",
    "    df_proc[\"channel_norm\"] = df_proc[channel_col].astype(str).map(normalize_text).replace({\"nan\": np.nan})\n",
    "\n",
    "if views_col and views_col in df_proc.columns:\n",
    "    df_proc[\"views_norm\"] = pd.to_numeric(df_proc[views_col], errors=\"coerce\")\n",
    "    df_proc[\"views_bucket\"] = pd.cut(df_proc[\"views_norm\"], bins=[-1,0,10,100,1000,10000,1e9],\n",
    "                                     labels=[\"0\",\"1-10\",\"11-100\",\"101-1k\",\"1k-10k\",\">10k\"])\n",
    "\n",
    "if reactions_col and reactions_col in df_proc.columns:\n",
    "    df_proc[\"reactions_norm\"] = pd.to_numeric(df_proc[reactions_col], errors=\"coerce\")\n",
    "\n",
    "if date_col:\n",
    "    df_proc[\"date_parsed_ist\"] = safe_to_datetime(df_proc[date_col])\n",
    "    dt = df_proc[\"date_parsed_ist\"]\n",
    "    df_proc[\"date\"] = dt.dt.date\n",
    "    df_proc[\"hour\"] = dt.dt.hour\n",
    "    df_proc[\"dow\"] = dt.dt.dayofweek\n",
    "    df_proc[\"month\"] = dt.dt.month\n",
    "    df_proc[\"year\"] = dt.dt.year\n",
    "\n",
    "# --- Trading signal extraction ---\n",
    "ACTION_RE = re.compile(r\"\\b(buy|sell|long|short)\\b\", flags=re.I)\n",
    "TARGETS_RE = re.compile(r\"target[s]?\\s*[-:]\\s*([\\d\\s\\-\\‚Äì,]+)\", flags=re.I)\n",
    "SL_RE = re.compile(r\"stop\\s*loss\\s*[-:]\\s*([\\d\\s\\-\\‚Äì,]+)\", flags=re.I)\n",
    "STRIKE_RE = re.compile(r\"\\b(\\d{3,6})\\s*(?:CE|PE)?\\b\", flags=re.I)\n",
    "\n",
    "def extract_trade_info(text):\n",
    "    data = {\"action\": None, \"strikes\": [], \"targets\": [], \"stoploss\": []}\n",
    "    if not isinstance(text, str): return data\n",
    "    m = ACTION_RE.search(text)\n",
    "    if m: data[\"action\"] = m.group(1).lower()\n",
    "    data[\"strikes\"] = STRIKE_RE.findall(text)\n",
    "    t = TARGETS_RE.search(text)\n",
    "    if t: data[\"targets\"] = re.findall(r\"\\d{2,6}\", t.group(1))\n",
    "    s = SL_RE.search(text)\n",
    "    if s: data[\"stoploss\"] = re.findall(r\"\\d{1,6}\", s.group(1))\n",
    "    return data\n",
    "\n",
    "trade_info = df_proc[\"text_norm\"].map(extract_trade_info)\n",
    "df_proc[\"trade_action\"] = trade_info.map(lambda d: d[\"action\"])\n",
    "df_proc[\"trade_strikes\"] = trade_info.map(lambda d: \",\".join(d[\"strikes\"]))\n",
    "df_proc[\"trade_targets\"] = trade_info.map(lambda d: \",\".join(d[\"targets\"]))\n",
    "df_proc[\"trade_stoploss\"] = trade_info.map(lambda d: \",\".join(d[\"stoploss\"]))\n",
    "\n",
    "# --- De-duplication ---\n",
    "dedupe_keys = [c for c in [\"channel_norm\",\"sender_norm\",\"text_norm\",\"date_parsed_ist\"] if c in df_proc.columns]\n",
    "if dedupe_keys:\n",
    "    before = len(df_proc)\n",
    "    df_proc = df_proc.drop_duplicates(subset=dedupe_keys, keep=\"first\").reset_index(drop=True)\n",
    "    print(f\"Dropped duplicates: {before - len(df_proc)} based on {dedupe_keys}\")\n",
    "\n",
    "# --- (Optional) fuzzy dedupe by similarity within channel+date ---\n",
    "def dedupe_group_by_similarity(df_group, text_col=\"text_for_model\", threshold=0.88):\n",
    "    docs = df_group[text_col].fillna(\"\").astype(str).tolist()\n",
    "    idxs = df_group.index.to_list()\n",
    "    n = len(docs)\n",
    "    if n <= 1: return df_group\n",
    "    vec = TfidfVectorizer(ngram_range=(1,2), max_features=5000, stop_words=\"english\")\n",
    "    X = vec.fit_transform(docs)\n",
    "    sim = linear_kernel(X, X)\n",
    "    to_drop = set()\n",
    "    for i in range(n):\n",
    "        if idxs[i] in to_drop: continue\n",
    "        dup_j = np.where(sim[i] > threshold)[0]\n",
    "        for j in dup_j:\n",
    "            if j > i: to_drop.add(idxs[j])\n",
    "    return df_group.drop(index=list(to_drop)) if to_drop else df_group\n",
    "\n",
    "result_frames = []\n",
    "max_group = 500\n",
    "if \"channel_norm\" in df_proc.columns and \"date\" in df_proc.columns:\n",
    "    for (chan,date), grp in df_proc.groupby([\"channel_norm\",\"date\"]):\n",
    "        if len(grp) <= 1:\n",
    "            result_frames.append(grp); continue\n",
    "        if len(grp) > max_group:\n",
    "            result_frames.append(grp); continue\n",
    "        cleaned = dedupe_group_by_similarity(grp)\n",
    "        result_frames.append(cleaned)\n",
    "    df_proc = pd.concat(result_frames, ignore_index=False)\n",
    "    print(\"After fuzzy dedupe rows:\", len(df_proc))\n",
    "\n",
    "# --- Save final ---\n",
    "df_proc.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "print(\"Saved final preprocessed:\", OUTPUT_CSV)\n",
    "\n",
    "# --- Save promo sample for manual labeling ---\n",
    "promo_sample = df_proc[df_proc[\"is_promo\"]].sample(n=min(1000, df_proc[df_proc[\"is_promo\"]].shape[0]), random_state=42)\n",
    "promo_sample.to_csv(PROMO_SAMPLE_CSV, index=False, encoding=\"utf-8\")\n",
    "print(\"Saved promo sample:\", PROMO_SAMPLE_CSV)\n",
    "\n",
    "df_proc.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f297af5e-be12-40d9-afd2-75ea84eb98fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning IA registry: D:\\Darryl\\Coding\\s_p\\data\\processed\\sebi_investment_advisors_cleaned.csv\n",
      "IA rows -> 969\n",
      "Cleaning RA registry: D:\\Darryl\\Coding\\s_p\\data\\processed\\sebi_research_analysts_cleaned.csv\n",
      "RA rows -> 1707\n",
      "Saved cleaned IA -> D:\\Darryl\\Coding\\s_p\\data\\processed\\sebi_investment_advisors_cleaned_v2.csv\n",
      "Saved cleaned RA -> D:\\Darryl\\Coding\\s_p\\data\\processed\\sebi_research_analysts_cleaned_v2.csv\n",
      "IA sample cleaned (first 20 rows)\n",
      "                                                          name_original registration_no                                                           name_norm                                    name_norm_simple                                                          sebi_key\n",
      "                                                          KAVITHA MENON    INA000000037                                                       kavitha menon                                       kavitha menon                                       INA000000037||kavitha menon\n",
      "                                               PRAKASH CHANDRA PRAHARAJ    INA000000045                                            prakash chandra praharaj                            prakash chandra praharaj                            INA000000045||prakash chandra praharaj\n",
      "                                     VALUEFY SOLUTIONS  PRIVATE LIMITED    INA000000060                                   valuefy solutions private limited                                   valuefy solutions                                   INA000000060||valuefy solutions\n",
      "                                               ICICI SECURITIES LIMITED    INA000000094                                            icici securities limited                                    icici securities                                    INA000000094||icici securities\n",
      "                                         ASTEYA INVESTMENT MANAGERS LLP    INA000000276                                      asteya investment managers llp                          asteya investment managers                          INA000000276||asteya investment managers\n",
      "                             SPT INVESTMENT ADVISORY SERVICES PVT. LTD.    INA000000326                           spt investment advisory services pvt ltd                     spt investment advisory services                    INA000000326||spt investment advisory services\n",
      "                                    V R WEALTH ADVISORS PRIVATE LIMITED    INA000000383                                 v r wealth advisors private limited                                 v r wealth advisors                                 INA000000383||v r wealth advisors\n",
      "                            BARCLAYS SECURITIES (INDIA) PRIVATE LIMITED    INA000000391                           barclays securities india private limited                           barclays securities india                           INA000000391||barclays securities india\n",
      "                             PLAN AHEAD WEALTH ADVISORS PRIVATE LIMITED    INA000000409                          plan ahead wealth advisors private limited                          plan ahead wealth advisors                          INA000000409||plan ahead wealth advisors\n",
      "                                                       SANDIP SABHARWAL    INA000000425                                                    sandip sabharwal                                    sandip sabharwal                                    INA000000425||sandip sabharwal\n",
      "                                                           GORAKH KADAM    INA000000441                                                        gorakh kadam                                        gorakh kadam                                        INA000000441||gorakh kadam\n",
      "                                      WEALTH FIRST ADVISORS PRIVATE LTD    INA000000482                                   wealth first advisors private ltd                               wealth first advisors                               INA000000482||wealth first advisors\n",
      "                                    ASK Wealth Advisors Private Limited    INA000000532                                 ask wealth advisors private limited                                 ask wealth advisors                                 INA000000532||ask wealth advisors\n",
      "TRUSTPLUTUS FAMILY OFFICE & INVESTMENT ADVISERS (INDIA) PRIVATE LIMITED    INA000000557 trustplutus family office investment advisers india private limited trustplutus family office investment advisers india INA000000557||trustplutus family office investment advisers india\n",
      "                                                AXIS SECURITIES LIMITED    INA000000615                                             axis securities limited                                     axis securities                                     INA000000615||axis securities\n",
      "                                          IIFL Capital Services Limited    INA000000623                                       iifl capital services limited                               iifl capital services                               INA000000623||iifl capital services\n",
      "                                                         AJIT SINGH DUA    INA000000631                                                      ajit singh dua                                      ajit singh dua                                      INA000000631||ajit singh dua\n",
      "                                 Purnartha Investment Advisers Pvt.Ltd.    INA000000672                              purnartha investment advisers pvt ltd                        purnartha investment advisers                       INA000000672||purnartha investment advisers\n",
      "                           QUANTUM INFORMATION SERVICES PRIVATE LIMITED    INA000000680                        quantum information services private limited                        quantum information services                        INA000000680||quantum information services\n",
      "                                                JIGNESH HARSHADRAI SHAH    INA000000789                                             jignesh harshadrai shah                             jignesh harshadrai shah                             INA000000789||jignesh harshadrai shah\n",
      "RA sample cleaned (first 20 rows)\n",
      "                                         name_original registration_no                                              name_norm                               name_norm_simple                                                     sebi_key\n",
      "                     STAKEHOLDERS EMPOWERMENT SERVICES    INH000000016                      stakeholders empowerment services              stakeholders empowerment services              INH000000016||stakeholders empowerment services\n",
      "INSTITUTIONAL INVESTOR ADVISORY SERVICES INDIA LIMITED    INH000000024 institutional investor advisory services india limited institutional investor advisory services india INH000000024||institutional investor advisory services india\n",
      "                                  DEEPAK KUMAR KANODIA    INH000000032                                   deepak kumar kanodia                           deepak kumar kanodia                           INH000000032||deepak kumar kanodia\n",
      "                           BOB CAPITAL MARKETS LIMITED    INH000000040                            bob capital markets limited                            bob capital markets                            INH000000040||bob capital markets\n",
      "                          GEPL CAPITAL PRIVATE LIMITED    INH000000081                           gepl capital private limited                                   gepl capital                                   INH000000081||gepl capital\n",
      "                                           PARAG SALOT    INH000000115                                            parag salot                                    parag salot                                    INH000000115||parag salot\n",
      "                          DAM CAPITAL ADVISORS LIMITED    INH000000131                           dam capital advisors limited                           dam capital advisors                           INH000000131||dam capital advisors\n",
      "                                     Angel One Limited    INH000000164                                      angel one limited                                      angel one                                      INH000000164||angel one\n",
      "                          ASHIKA STOCK BROKING LIMITED    INH000000206                           ashika stock broking limited                           ashika stock broking                           INH000000206||ashika stock broking\n",
      "                 CHOICE EQUITY BROKING PRIVATE LIMITED    INH000000222                  choice equity broking private limited                          choice equity broking                          INH000000222||choice equity broking\n",
      "                         IIFL Capital Services Limited    INH000000248                          iifl capital services limited                          iifl capital services                          INH000000248||iifl capital services\n",
      "     INVESTEC CAPITAL SERVICES (INDIA) PRIVATE LIMITED    INH000000263        investec capital services india private limited                investec capital services india                INH000000263||investec capital services india\n",
      "                   Prabhudas Lilladher Private Limited    INH000000271                    prabhudas lilladher private limited                            prabhudas lilladher                            INH000000271||prabhudas lilladher\n",
      "                               AXIS SECURITIES LIMITED    INH000000297                                axis securities limited                                axis securities                                INH000000297||axis securities\n",
      "                         AMBIT CAPITAL PRIVATE LIMITED    INH000000313                          ambit capital private limited                                  ambit capital                                  INH000000313||ambit capital\n",
      "               EMKAY GLOBAL FINANCIAL SERVICES LIMITED    INH000000354                emkay global financial services limited                emkay global financial services                INH000000354||emkay global financial services\n",
      "              Motilal Oswal Financial Services Limited    INH000000412               motilal oswal financial services limited               motilal oswal financial services               INH000000412||motilal oswal financial services\n",
      "                    ANIL KUMAR ASNANI PROP. SMART VERC    INH000000420                      anil kumar asnani prop smart verc              anil kumar asnani prop smart verc              INH000000420||anil kumar asnani prop smart verc\n",
      "        CITIGROUP GLOBAL MARKETS INDIA PRIVATE LIMITED    INH000000438         citigroup global markets india private limited                 citigroup global markets india                 INH000000438||citigroup global markets india\n",
      "                         BofA Securities India Limited    INH000000503                          bofa securities india limited                          bofa securities india                          INH000000503||bofa securities india\n",
      "\n",
      "Quick stats:\n",
      "IA rows: 969 unique name_norm_simple: 968\n",
      "RA rows: 1707 unique name_norm_simple: 1699\n",
      "\n",
      "Files written to /mnt/data:\n",
      " - D:\\Darryl\\Coding\\s_p\\data\\processed\\sebi_investment_advisors_cleaned_v2.csv (exists: True , size: 526225 )\n",
      " - D:\\Darryl\\Coding\\s_p\\data\\processed\\sebi_research_analysts_cleaned_v2.csv (exists: True , size: 908027 )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darryl\\AppData\\Local\\Temp\\ipykernel_8060\\1348016933.py:71: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[date_col + \"_parsed\"] = pd.to_datetime(df[date_col].astype(str), errors=\"coerce\", utc=True).dt.tz_convert(None)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import re, unicodedata, math, os\n",
    "from datetime import datetime\n",
    "# use display helper if available in your environment (optional)\n",
    "try:\n",
    "    from caas_jupyter_tools import display_dataframe_to_user\n",
    "    _HAS_DISPLAY = True\n",
    "except Exception:\n",
    "    _HAS_DISPLAY = False\n",
    "\n",
    "# **INPUT**: replace these if your files are in another path\n",
    "ia_in = r\"D:\\Darryl\\Coding\\s_p\\data\\processed\\sebi_investment_advisors_cleaned.csv\"\n",
    "ra_in = r\"D:\\Darryl\\Coding\\s_p\\data\\processed\\sebi_research_analysts_cleaned.csv\"\n",
    "\n",
    "# **OUTPUT** (cleaned files written here)\n",
    "ia_out = r\"D:\\Darryl\\Coding\\s_p\\data\\processed\\sebi_investment_advisors_cleaned_v2.csv\"\n",
    "ra_out = r\"D:\\Darryl\\Coding\\s_p\\data\\processed\\sebi_research_analysts_cleaned_v2.csv\"\n",
    "\n",
    "# Helper functions for normalization\n",
    "def normalize_text_for_matching(s: str) -> str:\n",
    "    \"\"\"Lowercase, remove most punctuation (keep Devanagari letters), collapse spaces.\"\"\"\n",
    "    if s is None or (isinstance(s, float) and math.isnan(s)):\n",
    "        return \"\"\n",
    "    if not isinstance(s, str):\n",
    "        s = str(s)\n",
    "    s = s.strip()\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "    s = re.sub(r\"[^\\w\\s\\u0900-\\u097F]\", \" \", s, flags=re.UNICODE)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.lower()\n",
    "\n",
    "def simplify_org_suffixes(s: str) -> str:\n",
    "    \"\"\"Remove common company suffixes to make name matching simpler.\"\"\"\n",
    "    if not s:\n",
    "        return s\n",
    "    s2 = re.sub(r\"\\b(pvt|pvt\\.|ltd|ltd\\.|private|limited|llp|inc|corp|co|company|pvtltd|pvtltd)\\b\", \" \", s, flags=re.I)\n",
    "    s2 = re.sub(r\"\\s+\", \" \", s2).strip()\n",
    "    return s2\n",
    "\n",
    "def clean_registry(path: str, name_col: str = \"name\") -> pd.DataFrame:\n",
    "    \"\"\"Load CSV as strings, normalize fields, parse dates, return cleaned DataFrame.\"\"\"\n",
    "    df = pd.read_csv(path, dtype=str, keep_default_na=False, na_values=[\"\", \"nan\", \"NaN\"])\n",
    "    if name_col not in df.columns:\n",
    "        name_col = df.columns[0]\n",
    "    # canonical columns\n",
    "    df[\"name_original\"] = df[name_col].astype(str).fillna(\"\")\n",
    "    # find registration column heuristically\n",
    "    reg_col = None\n",
    "    for c in df.columns:\n",
    "        if any(k in c.lower() for k in (\"registration_no\",\"reg_no\",\"registration\",\"registration_no\",\"regno\")):\n",
    "            reg_col = c\n",
    "            break\n",
    "    if reg_col is not None:\n",
    "        df[\"registration_no\"] = df[reg_col].astype(str).str.strip().str.upper().replace({\"nan\": \"\"})\n",
    "    else:\n",
    "        df[\"registration_no\"] = \"\"\n",
    "    # ensure phone columns are strings (avoid scientific notation)\n",
    "    phone_cols = [c for c in df.columns if any(k in c.lower() for k in (\"phone\",\"telephone\",\"mobile\"))]\n",
    "    for c in phone_cols:\n",
    "        df[c] = df[c].astype(str).replace({\"nan\": \"\"}).str.strip()\n",
    "    # name normalizations (two flavors)\n",
    "    df[\"name_norm\"] = df[\"name_original\"].apply(normalize_text_for_matching)\n",
    "    df[\"name_norm_simple\"] = df[\"name_norm\"].apply(simplify_org_suffixes)\n",
    "    df[\"sebi_key\"] = df[\"registration_no\"].fillna(\"\") + \"||\" + df[\"name_norm_simple\"].fillna(\"\")\n",
    "\n",
    "    # robust date parsing (parse common date columns, make tz-safe)\n",
    "    for date_col in (\"reg_date\", \"registration_date\", \"expiry_date\", \"expirty_date\", \"to\"):\n",
    "        if date_col in df.columns:\n",
    "            try:\n",
    "                # parse via UTC then drop tz to get consistent naive datetimes\n",
    "                df[date_col + \"_parsed\"] = pd.to_datetime(df[date_col].astype(str), errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "            except Exception:\n",
    "                try:\n",
    "                    df[date_col + \"_parsed\"] = pd.to_datetime(df[date_col].astype(str), errors=\"coerce\")\n",
    "                except Exception:\n",
    "                    df[date_col + \"_parsed\"] = pd.NaT\n",
    "\n",
    "    # compute days to expiry (if expiry parsed)\n",
    "    if \"expiry_date_parsed\" in df.columns:\n",
    "        try:\n",
    "            today = pd.Timestamp.utcnow().normalize()\n",
    "            df[\"days_to_expiry_v2\"] = (df[\"expiry_date_parsed\"] - today).dt.days\n",
    "        except Exception:\n",
    "            df[\"days_to_expiry_v2\"] = pd.NA\n",
    "    else:\n",
    "        df[\"days_to_expiry_v2\"] = pd.NA\n",
    "\n",
    "    return df\n",
    "\n",
    "# Run cleaning\n",
    "print(\"Cleaning IA registry:\", ia_in)\n",
    "df_ia_clean = clean_registry(ia_in, name_col=\"name\")\n",
    "print(\"IA rows ->\", len(df_ia_clean))\n",
    "\n",
    "print(\"Cleaning RA registry:\", ra_in)\n",
    "df_ra_clean = clean_registry(ra_in, name_col=\"name\")\n",
    "print(\"RA rows ->\", len(df_ra_clean))\n",
    "\n",
    "# Save outputs\n",
    "df_ia_clean.to_csv(ia_out, index=False)\n",
    "df_ra_clean.to_csv(ra_out, index=False)\n",
    "print(\"Saved cleaned IA ->\", ia_out)\n",
    "print(\"Saved cleaned RA ->\", ra_out)\n",
    "\n",
    "# Show quick sample + stats (display helper used if available)\n",
    "def show_sample(df, title):\n",
    "    cols = [\"name_original\",\"registration_no\",\"name_norm\",\"name_norm_simple\",\"sebi_key\"]\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "    sample = df.head(20)[cols]\n",
    "    if _HAS_DISPLAY:\n",
    "        display_dataframe_to_user(title, sample)\n",
    "    else:\n",
    "        print(title)\n",
    "        print(sample.to_string(index=False))\n",
    "\n",
    "show_sample(df_ia_clean, \"IA sample cleaned (first 20 rows)\")\n",
    "show_sample(df_ra_clean, \"RA sample cleaned (first 20 rows)\")\n",
    "\n",
    "print(\"\\nQuick stats:\")\n",
    "print(\"IA rows:\", len(df_ia_clean), \"unique name_norm_simple:\", df_ia_clean[\"name_norm_simple\"].nunique())\n",
    "print(\"RA rows:\", len(df_ra_clean), \"unique name_norm_simple:\", df_ra_clean[\"name_norm_simple\"].nunique())\n",
    "\n",
    "print(\"\\nFiles written to /mnt/data:\")\n",
    "for p in [ia_out, ra_out]:\n",
    "    print(\" -\", p, \"(exists:\", os.path.exists(p), \", size:\", os.path.getsize(p) if os.path.exists(p) else 'N/A', \")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c78cd0f-dcd6-4a13-a304-5cdfed9cdf31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (SEBI Project)",
   "language": "python",
   "name": "sebi_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
